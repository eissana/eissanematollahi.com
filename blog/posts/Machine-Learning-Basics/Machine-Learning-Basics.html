<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2018-08-22 Wed 20:25 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Machine Learning Basics</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Ubuntu" />
<style type="text/css">
<!--/*--><![CDATA[/*><!--*/
html {
    font-family: sans-serif;
    -ms-text-size-adjust: 100%;
    -webkit-text-size-adjust: 100%
}
body {
    margin: 0;
}
#table-of-contents {
    z-index: 1;
}
/* TOC inspired by http://jashkenas.github.com/coffee-script */

#table-of-contents {
    font-size: 10pt;
    position: fixed;
    right: 0em;
    top: 0em;
    background: white;
    -webkit-box-shadow: 0 0 1em #777777;
    -moz-box-shadow: 0 0 1em #777777;
    -webkit-border-bottom-left-radius: 5px;
    -moz-border-radius-bottomleft: 5px;
    text-align: right;
    /* ensure doesn't flow off the screen when expanded */

    max-height: 80%;
    overflow: auto;
}
#table-of-contents h2 {
    font-size: 10pt;
    max-width: 8em;
    font-weight: normal;
    padding-left: 0.5em;
    padding-left: 0.5em;
    padding-top: 0.05em;
    padding-bottom: 0.05em;
}
#table-of-contents #text-table-of-contents {
    display: none;
    text-align: left;
}
#table-of-contents:hover #text-table-of-contents {
    display: block;
    padding: 0.5em;
    margin-top: -1.5em;
}
article,
aside,
details,
figcaption,
figure,
footer,
header,
hgroup,
main,
menu,
nav,
section,
summary {
    display: block
}
audio,
canvas,
progress,
video {
    display: inline-block;
    vertical-align: baseline
}
audio:not([controls]) {
    display: none;
    height: 0
}
[hidden],
template {
    display: none
}
a {
    background-color: transparent
}
a:active,
a:hover {
    outline: 0
}
abbr[title] {
    border-bottom: 1px dotted
}
b,
strong {
    font-weight: 700
}
dfn {
    font-style: italic
}
h1 {
    font-size: 2em;
    margin: .67em 0
}
mark {
    background: #ff0;
    color: #000
}
small {
    font-size: 80%
}
sub,
sup {
    font-size: 75%;
    line-height: 0;
    position: relative;
    vertical-align: baseline
}
sup {
    top: -.5em
}
sub {
    bottom: -.25em
}
img {
    border: 0
}
svg:not(:root) {
    overflow: hidden
}
figure {
    margin: 1em 40px
}
hr {
    box-sizing: content-box;
    height: 0
}
pre {
    overflow: auto
}
code,
kbd,
pre,
samp {
    font-family: monospace;
    font-size: 1em
}
button,
input,
optgroup,
select,
textarea {
    color: inherit;
    font: inherit;
    margin: 0
}
button {
    overflow: visible
}
button,
select {
    text-transform: none
}
button,
html input[type=button],
input[type=reset],
input[type=submit] {
    -webkit-appearance: button;
    cursor: pointer
}
button[disabled],
html input[disabled] {
    cursor: default
}
button::-moz-focus-inner,
input::-moz-focus-inner {
    border: 0;
    padding: 0
}
input {
    line-height: normal
}
input[type=checkbox],
input[type=radio] {
    box-sizing: border-box;
    padding: 0
}
input[type=number]::-webkit-inner-spin-button,
input[type=number]::-webkit-outer-spin-button {
    height: auto
}
input[type=search] {
    -webkit-appearance: textfield;
    box-sizing: content-box
}
input[type=search]::-webkit-search-cancel-button,
input[type=search]::-webkit-search-decoration {
    -webkit-appearance: none
}
fieldset {
    border: 1px solid silver;
    margin: 0 2px;
    padding: .35em .625em .75em
}
legend {
    border: 0;
    padding: 0
}
textarea {
    overflow: auto
}
optgroup {
    font-weight: 700
}
table {
    border-collapse: collapse;
    border-spacing: 0
}
td,
th {
    padding: 0
}
body {
    width: 95%;
    margin: 2%;
    font: normal normal normal 16px/1.6em Open Sans, sans;
    color: #333
}
@media (min-width: 1025px) {
    body {
        width: 900px;
        margin-left: 160px;
        font-family: "Lato","Helvetica Neue",Helvetica,Arial,sans-serif;
        font-size: 15px;
    }
}
.title {
    padding: .5em 0;
    color: #000
}
.subtitle,
.title {
    text-align: center
}
.subtitle {
    font-size: medium;
    font-weight: 700
}
.abstract {
    margin: auto;
    width: 80%;
    font-style: italic
}
.abstract p:last-of-type:before {
    content: "    ";
    white-space: pre
}
.status {
    font-size: 90%;
    margin: 2em auto
}
[class^=section-number-] {
    margin-right: .5em
}
#footnotes {
    font-size: 90%
}
.footpara {
    display: inline;
    margin: .2em auto
}
.footdef {
    margin-bottom: 1em
}
.footdef sup {
    padding-right: .5em
}
a {
    color: #527d9a;
    text-decoration: none
}
a:hover {
    color: #035;
    border-bottom: 1px dotted
}
img {
    max-width: 100%;
    vertical-align: middle
}
.MathJax_Display {
    margin: 0!important;
    width: 90%!important
}
h1,
h2,
h3,
h4,
h5,
h6 {
    color: #a5573e;
    line-height: 1.6em;
    font-family: Droid Serif, serif
}
h4,
h5,
h6 {
    font-size: 1em
}
dt {
    font-weight: 700
}
table {
    margin: auto;
    border-top: 2px solid;
    border-collapse: collapse
}
table,
thead {
    border-bottom: 2px solid
}
table td+td,
table th+th {
    border-left: 1px solid gray
}
table tr {
    border-top: 1px solid #d3d3d3
}
td,
th {
    padding: 5px 10px;
    vertical-align: middle
}
caption.t-above {
    caption-side: top
}
caption.t-bottom {
    caption-side: bottom
}
th.org-center,
th.org-left,
th.org-right {
    text-align: center
}
td.org-right {
    text-align: right
}
td.org-left {
    text-align: left
}
td.org-center {
    text-align: center
}
code {
    padding: 2px 5px;
    margin: auto 1px;
    /* border: 1px solid #ddd; */
    border-radius: 3px;
    background-clip: padding-box;
    color: #333;
    font-size: 90%
}
blockquote {
    margin: 1em 2em;
    padding-left: 1em;
    border-left: 3px solid #ccc
}
kbd {
    background-color: #f7f7f7;
    font-size: 90%;
    margin: 0 .1em;
    padding: .1em .6em
}
.figure {
    padding: 1em
}
.figure p {
    text-align: center
}
.todo {
    color: red
}
.done,
.todo {
    font-family: monospace
}
.done {
    color: green
}
.priority {
    color: orange
}
.priority,
.tag {
    font-family: monospace
}
.tag {
    background-color: #eee;
    font-size: 80%;
    font-weight: 400;
    padding: 2px
}
.timestamp {
    color: #bebebe
}
.timestamp-kwd {
    color: #5f9ea0
}
.org-right {
    margin-left: auto;
    margin-right: 0;
    text-align: right
}
.org-left {
    margin-left: 0;
    margin-right: auto;
    text-align: left
}
.org-center {
    margin-left: auto;
    margin-right: auto;
    text-align: center
}
.underline {
    text-decoration: underline
}
#postamble p,
#preamble p {
    font-size: 90%;
    margin: .2em
}
p.verse {
    margin-left: 3%
}
pre {
    background-color: #2b3e50;
    color: #eee;
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    font-family: Menlo, Pragmata, 'DejaVu LGC Sans Mono', 'DejaVu Sans Mono', Consolas, 'Everson Mono', 'Lucida Console', 'Andale Mono', 'Nimbus Mono L', 'Liberation Mono', FreeMono, 'Osaka Monospaced', Courier, 'New Courier', monospace;
    margin: 1.2em;
    padding: 8pt
}
pre.src {
    overflow: auto;
    padding-top: 1.2em;
    position: relative;
    font-size: 90%
}
pre.src:before {
    background-color: #fff;
    border: 1px solid #000;
    display: none;
    padding: 3px;
    position: absolute;
    right: 10px;
    top: .6em
}
pre.src:hover:before {
    display: inline
}
pre.src-sh:before {
    content: 'sh'
}
pre.src-bash:before {
    content: 'bash'
}
pre.src-emacs-lisp:before {
    content: 'Emacs Lisp'
}
pre.src-R:before {
    content: 'R'
}
pre.src-org:before {
    content: 'Org'
}
pre.src-c+:before {
    content: 'C++'
}
pre.src-c:before {
    content: 'C'
}
pre.src-html:before {
    content: 'HTML'
}
.inlinetask {
    background: #ffc;
    border: 2px solid gray;
    margin: 10px;
    padding: 10px
}
#org-div-home-and-up {
    font-size: 70%;
    text-align: right;
    white-space: nowrap
}
.linenr {
    font-size: smaller
}
.code-highlighted {
    background-color: #ff0
}
#bibliography {
    font-size: 90%
}
#bibliography table {
    width: 100%
}
.creator {
    display: block
}
@media (min-width: 1025px) {
    .creator {
        display: inline;
        float: right
    }
}/*]]>*/-->
</style>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2018 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Machine Learning Basics
<br />
<span class="subtitle">Better understanding of the most common questions machine learning</span>
</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org67ac68a">Introduction</a></li>
<li><a href="#org8ef91ca">Machine Learning Models</a></li>
<li><a href="#org3d7cecc">Best Prediction Model</a></li>
<li><a href="#orgf2ad9c1">Bias-Variance Tradeoff</a></li>
<li><a href="#org0f45ca9">Cross-Validation</a></li>
</ul>
</div>
</div>

<div id="outline-container-org67ac68a" class="outline-2">
<h2 id="org67ac68a">Introduction</h2>
<div class="outline-text-2" id="text-org67ac68a">
<p>
Machine learning is a statistical approach for making data-driven decisions through building models that are capable of capturing some patterns in data. Using historical data, models are <i>trained</i> to <i>learn</i> some patterns expected to exist in unseen data as well. Such machine learning models can answer questions of the following types:
</p>
<ul class="org-ul">
<li>Is a transaction fraudulent? (classification)</li>
<li>Is an email spam? (classification)</li>
<li>What is the likelihood of a patient having cancer? (classification)</li>
<li>How much will the price of a stock be tomorrow? (regression)</li>
<li>Does a group of news belong to the same category? (clustering)</li>
<li>Are beer and diaper bought together frequently? (association rules)</li>
</ul>

<p>
A common approach to build a machine learning model is depicted in the following diagram. This workflow is explained in more details next.
</p>


<div class="figure">
<p><img src="images/machine-learning-workflow.png" alt="machine-learning-workflow.png" />
</p>
</div>

<dl class="org-dl">
<dt>Representation</dt><dd>In this step, we collect data and choose a class of algorithms suitable for our particular problem. This class of algorithms can be a collection of k-nearest neighbors classifiers with different k parameter. We aim to choose the best from the set of models.</dd>
<dt>Evaluation</dt><dd>The collected data is partitioned into the <i>training</i>, <i>validation</i>, and <i>test</i> data. We train models on the training data and evaluate them on the validation data. The model with the best performance on the validation data is (temporarily) selected.</dd>
<dt>Optimization</dt><dd>We tune parameters of the model and repeat the evaluation step until we get satisfactory performance on the validation data. The performance of the final model is computed using the test data.</dd>
</dl>
<p>
In summary, training data is used to train and build a model. Validation data is used to fine-tune the model. Finally, test data is used to compute the selected model&rsquo;s performance.
</p>
</div>
</div>

<div id="outline-container-org8ef91ca" class="outline-2">
<h2 id="org8ef91ca">Machine Learning Models</h2>
<div class="outline-text-2" id="text-org8ef91ca">
<p>
Machine learning algorithms look for hidden patterns in historical data with the <i>hope</i> that the patterns will exist in future unseen data. Machine learning models, or <i>learners</i>, are then built to capture the patterns and enable us to answer questions of interest by applying the models to unseen data. Machine learning algorithms include two major categories: <b>supervised</b> and <b>unsupervised</b>.
</p>

<p>
In machine learning, data is typically a set of <b>records</b>, each containing a set of <b>features</b>. For example, in a clinical study, a record may represent information of a patient, while a feature may represent blood pressure of the patient. [include a table of real clinical data] 
</p>

<p>
Value of the features can be <b>scalar</b>, <b>categorical</b>, <b>ordinal</b>, or <b>text</b>. 
</p>
<ul class="org-ul">
<li>Scalar features typically contain measurement values, such as price, age, height, blood pressure, etc.</li>
<li>Categorical features hold categories of a small set. For example, spam filtering data may have a <i>Spam</i> feature with values {YES, NO} to indicate whether or not an email (a record of spam data) is spam.</li>
<li>Ordinal features are the same as categorical features in which categories are ordered. For example, T-shirt sizes may consist of the values {SMALL, MEDIUM, LARGE} which define an order.</li>
<li>Text features can hold any data in the text format, such as name, home address, date, etc.</li>
</ul>

<p>
In supervised learning, there is an outcome feature, beside other features, which is typically categorical or scalar. The goal is to predict the value of the outcome feature given the values of other features. The values of the outcome feature are given for historical data to <i>supervise</i> training prediction models or learners. For example, given a set of emails, we can manually categorize them as spam or not-spam. Then, we build a model using some features of the emails along with their outcome values (spam/not-spam). The model can finally be used to categorize a new email as spam or not-spam. This problem is a typical example of the <i>binary classification</i> problem. other types include the <i>regression problem</i> and the <i>multi-class classification problem</i>.
</p>

<p>
In an unsupervised learning, there is no outcome feature, and the objective is find hidden relations among records. For example, given a set of news, we may want to know how to organize them into a few clusters. This is referred to as the <i>clustering problem</i>. As another example, suppose we are given a set of transactions in a store, and would like to discover subsets of items that are bought together. This problem is referred to as the <i>market basket analysis</i> or <i>association rules</i>.
</p>

<p>
In the next section, we will see how to formulate the supervised learning problem as an optimization model. 
</p>
</div>
</div>

<div id="outline-container-org3d7cecc" class="outline-2">
<h2 id="org3d7cecc">Best Prediction Model</h2>
<div class="outline-text-2" id="text-org3d7cecc">
<p>
A supervised machine learning problem (whether classification or regression) is to find a parametric function, a.k.a <i>model</i>, that reliably predicts target values of unseen data. To define the <i>best</i> model, we need a measure to compare different models. A loss functions is often used to measure the prediction error of a model. Therefore, a machine learning problem can be cast as an optimization problem to find model parameters that minimize the loss function. Note that we build a model using training data, thus called training a model, but compare models using test data to see how they can generalize to unseen data. Model accuracy and generalizability are both important and will be discussed in more details in the next section.
</p>

<p>
To formulate the optimization problem, let us focus on the regression problem first and introduce some notation. Let \(X\) be a set of \(m\) records. Record \(i\) of \(X\) is composed of \(n\) features as \(x_{i:}=(x_{i1},\ldots,x_{in})\). Moreover, let \(y=(y_1,\ldots,y_m)\) denote the target vector. The set of records along with its associated target vector can be depicted in a tabular form as in Table <a href="#orgbb8081a">1</a>. A generic record of \(X\) is denoted by \(x\).
</p>

<table id="orgbb8081a" border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption class="t-above"><span class="table-number">Table 1:</span> Data table</caption>

<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">X</th>
<th scope="col" class="org-left">y</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">\(x_{11}\quad x_{12}\quad \dots \quad x_{1n}\)</td>
<td class="org-left">\(y_1\)</td>
</tr>

<tr>
<td class="org-left">\(\vdots\quad\quad \vdots\qquad\qquad \vdots\)</td>
<td class="org-left">\(\vdots\)</td>
</tr>

<tr>
<td class="org-left">\(x_{m1}\quad x_{m2}\quad \dots \quad x_{mn}\)</td>
<td class="org-left">\(y_m\)</td>
</tr>
</tbody>
</table>

<p>
A prediction model for the pair of input data \((X, y)\) is a parametric function \(\phi_w(x)\) which maps a record \(x\) from the features space to a value \(y\) in the target space. For example, \(\phi_w(x)=w^Tx+w_0\) is a linear model, where \(w\) and \(w_0\) are unknowns parameters. To find unknown parameters, we fit the model to a given set of training data with the objective of minimizing a proper loss function. For example, we can find unknown parameters by minimizing <i>sum of squared errors (SSE)</i> defined as:
\[ 
   \sum_{i=1}^n \left(y_i-\phi_w(x_{i:})\right)^2.
\]
</p>

<p>
In classification, we not only care about predicting class labels for unseen data, but also want to know how confident we are in predicting such. Therefore, we aim at predicting class probabilities or scores, from which class labels can be derived. For this, we may optimize the <b>maximum likelihood estimation (MLE)</b> or the <b>maximum a posteriori (MAP)</b> function, rather than optimizing loss functions directly. The <b>Expectation maximization(EM)</b> algorithm is often employed to solve MLE and MAP problems iteratively. 
</p>

<p>
A classification model may map \(x\) values to scores or probabilities from which class labels can be easily derived. Scores are real values in \((-\infty, +\infty)\), but probabilities are real values in \([0,1]\). We will denote score of \(x\) by \(\phi_w(x)\), probability of \(x\) by \(\psi_w(x)\), and predicted label of \(x\) by \(b_w(x)\). Assuming without loss of generality, that the class labels are \(\{-1,1\}\), the following approach is often used to compute predicted labels from scores or probabilities:
\[
  b_w(x)=\begin{cases}
          1\quad \phi_w(x) \ge 0,\\
          -1\quad \phi_w(x) < 0,
         \end{cases}
  \qquad\qquad
  b_w(x)=\begin{cases}
          1\quad \psi_w(x) \ge 0.5,\\
          -1\quad \psi_w(x) < 0.5.
         \end{cases}
\] 
</p>

<p>
Machine learning models typically have a complexity parameter. As model complexity increases, the prediction error on training data is expected to decrease. Although very accurate on training data, highly complex models are not generalizable to unseen data. Thus, there is a tradeoff between accuracy and generalizability of a model. In the next section, we will learn more about this tradeoff and characteristics of a good model.
</p>
</div>
</div>

<div id="outline-container-orgf2ad9c1" class="outline-2">
<h2 id="orgf2ad9c1">Bias-Variance Tradeoff</h2>
<div class="outline-text-2" id="text-orgf2ad9c1">
<p>
As we discussed in the previous section, the prediction error on training data is not enough to assess the goodness of a model. A good model needs to be generalizable to unseen data as well. In can be shown that the expected error of a model is composed of three terms: <i>bias</i>, <i>variance</i>, and an irreducible error term; consult with <a href="https://web.stanford.edu/~hastie/ElemStatLearn/">The Elements of Statistical Learning</a> for the proof and detailed discussion.
</p>

<p>
Bias is an error term that measures the <b>accuracy</b> of a model. High bias means that the model does not really capture the hidden pattern in the data. This is referred to as <b>under-fitting</b>. We ideally want a low bias model; but how low the bias should be? Models with a very low bias tend to capture the noise in the training data, resulting in an <b>over-fitted</b> model. Therefore, the bias itself as a measure is not enough for building a good model; we need another measure.
</p>

<p>
The variance is an error term that measures the <b>consistency</b> of a model. Over-fitted models usually have high variance. A high variance indicates that the model is not generalizable to unseen data.
</p>

<p>
Ideally, we want a model that captures hidden patterns in the training data (low bias) and generalizes well to unseen data (low variance). Thus, we need to minimize both bias and variance, simultaneously. As shown in Figure <a href="#orgd981fce">2</a>, a simple model usually has a high bias; such a model is under-fitted, regardless of having low or high variance. Assuming that we have enough training data, increasing model complexity will cause the bias and variance to decrease until a point where the variance will begin to grow. That point defines a model with optimal complexity that minimizes both bias and variance, simultaneously.
</p>


<div id="orgd981fce" class="figure">
<p><img src="./images/bias-variance-tradeoff.png" alt="bias-variance-tradeoff.png" />
</p>
<p><span class="figure-number">Figure 2: </span>Bias-variance tradeoff in machine learning. A simple model yields high bias (low accuracy) on both training and test data. A complex model, on the other hand, yields high variance (low consistency) as it captures noise in the training data, too.</p>
</div>

<p>
In summary, we have the following four cases, as depicted in Figure <a href="#orgd31ac5b">3</a>:
</p>
<ul class="org-ul">
<li>High bias, high variance: The model is both inaccurate and inconsistent: under-fitted model. Typically, this occurs when there is no enough training data. To avoid this case, we simply collect more data.</li>
<li>High bias, low variance: The model is consistently inaccurate: under-fitted model.</li>
<li>Low bias, high variance: The model is accurate but inconsistent: over-fitted model.</li>
<li>Low bias, low variance: The model is both accurate and consistent: well-fitted model.</li>
</ul>


<div id="orgd31ac5b" class="figure">
<p><img src="./images/bias-variance-dart.jpg" alt="bias-variance-dart.jpg" />
</p>
<p><span class="figure-number">Figure 3: </span>Bias-variance variation. A good model has both low bias and low variance. High bias indicates that the model in under-fitted, and high variance signals that the model is over-fitted.</p>
</div>

<p>
So far we learned that a good model, trained on the training data, has a low prediction error on the test data. However, we cannot rely on one set of training and test data, as we may get lucky to obtain low prediction error on one test data. In other words, one set of data is not representative of the whole space of possible unseen data. 
</p>

<p>
One solution is to collect many sample data and repeat the process to compute prediction errors and combine them to obtain a good estimate of the true prediction error of the model. One way to combine the prediction errors is to take the average of them.
</p>

<p>
The problem with the latter solution is that we may not be able to collect many sets of data. Cross-validation technique, discussed in the next section, is a well-known approach to generate multiple sets of training and validation data from a single data set.
</p>
</div>
</div>

<div id="outline-container-org0f45ca9" class="outline-2">
<h2 id="org0f45ca9">Cross-Validation</h2>
<div class="outline-text-2" id="text-org0f45ca9">
<p>
One of the most widely-used methods to estimate the prediction error is the <i>\(K\)-fold cross-validation</i>. This method divides data into \(K\) parts and generates \(K\) pairs of training-validation data as follows. For each \(k\in\{1,2,\ldots,K\}\), the \(k\)-th part in Set \(k\) is the validation data, while the rest is the training data. A 4-fold cross-validation data partitioning is depicted in the following diagram.
</p>


<div class="figure">
<p><img src="images/cross-validation.png" alt="cross-validation.png" />
</p>
</div>

<p>
To compute the prediction error of a machine learning algorithm, we build models on the training data sets and compute 
</p>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Date: 2018-08-17 Fri 00:00</p>
<p class="author">Author: Ubuntu</p>
<p class="date">Created: 2018-08-22 Wed 20:25</p>
<p class="validation"></p>
</div>
</body>
</html>
